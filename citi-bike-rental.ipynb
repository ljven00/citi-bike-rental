{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4efc6ecf",
   "metadata": {},
   "source": [
    "## Citi Bike NYC Data Engineering Project\n",
    "--------------------------------------------------\n",
    "\n",
    "### Project Overview\n",
    "\n",
    "This project focuses on building a clean, reproducible data pipeline for Citi Bike rental data. The objective is to take raw trip-level and weather datasets, apply systematic data cleaning and normalization steps, and load the resulting tables into a relational SQLite database with analytical views.\n",
    "\n",
    "- The notebook is designed with a data-engineering mindset rather than exploratory analysis.\n",
    "- All transformations are intentional and aligned with a predefined SQL schema.\n",
    "\n",
    "### Key goals of this project:\n",
    "\n",
    "- Clean and standardize raw trip and weather data\n",
    "- Normalize entities into dimension and fact tables\n",
    "- Enforce schema consistency between pandas and SQLite\n",
    "- Load validated data into a relational database for downstream analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed80a29e",
   "metadata": {},
   "source": [
    "##### Imports and Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c978fb29",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sqlite3\n",
    "from glob import glob\n",
    "from pathlib import Path\n",
    "\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "\n",
    "DB_PATH = Path(\"./db/citibike.db\")\n",
    "RIDE_Files = glob(\"./data/JC-2016*.csv\")\n",
    "WEATHER_FILE_PATH = Path(\"./data/newark_airport_2016.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "819eeb58",
   "metadata": {},
   "source": [
    "##### Load Raw Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2468733e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ride = pd.concat((pd.read_csv(file) for file in RIDE_Files), ignore_index=True)\n",
    "weather = pd.read_csv(WEATHER_FILE_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63b2084b",
   "metadata": {},
   "source": [
    "##### Initial Inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f465bbda",
   "metadata": {},
   "outputs": [],
   "source": [
    "ride.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "822e643f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ride.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d7878b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "ride.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a74e0c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "weather.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "646e2ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "weather.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a15b787",
   "metadata": {},
   "outputs": [],
   "source": [
    "weather.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0feebbd4",
   "metadata": {},
   "source": [
    "##### Data Cleaning\n",
    "\n",
    "##### Rename Columns (For Standardization Purposes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b83adc6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ride.columns = (\n",
    "    ride.columns\n",
    "    .str.strip()\n",
    "    .str.replace(\" \", \"_\")\n",
    "    .str.lower()\n",
    ")\n",
    "\n",
    "old_columns = weather.columns\n",
    "new_columns = [\"weather_station_id\", \"date\", \"avg_daily_wind_speed\", \"peak_gust_time\", \\\n",
    "            \"precipitation\", \"snowfall\", \"snow_depth\", \"avg_temp\", \"max_temp\",\\\n",
    "            \"min_temp\", \"daily_total_sushine\", \"fastest_2min_wind_dir\", \\\n",
    "            \"fastest_5min_wind_dir\", \"fastest_2min_wind_speed\", \\\n",
    "            \"fastest_5min_wind_speed\"]\n",
    "\n",
    "weather.rename(columns=dict(zip(old_columns, new_columns)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03a4726f",
   "metadata": {},
   "source": [
    "##### Data Type Conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94ec232e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ride[\"start_time\"] = pd.to_datetime(ride[\"start_time\"], errors=\"coerce\")\n",
    "ride[\"stop_time\"] = pd.to_datetime(ride[\"stop_time\"], errors=\"coerce\")\n",
    "ride[\"trip_duration\"] = pd.to_numeric(ride[\"trip_duration\"], errors=\"coerce\")\n",
    "ride[\"birth_year\"] = pd.to_numeric(ride[\"birth_year\"], errors=\"coerce\")\\\n",
    "    .astype(\"Int64\")\n",
    "\n",
    "weather[\"date\"] = pd.to_datetime(weather[\"date\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac5ce80e",
   "metadata": {},
   "source": [
    "##### Drop Null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "713c37bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows missing essential trip identifiers\n",
    "ride = ride.dropna(subset=[\n",
    "    \"start_time\",\n",
    "    \"start_station_id\",\n",
    "    \"bike_id\"\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e12d384c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop null columns in the weather dataframe\n",
    "weather = weather.drop(columns=[\"peak_gust_time\", \"daily_total_sushine\"], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "964ddba8",
   "metadata": {},
   "source": [
    "##### Standardize Categorical Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c21065a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lowercase and remove extra space if any\n",
    "ride[\"user_type\"] = ride[\"user_type\"].str.lower().str.strip()\n",
    "ride[\"gender\"] = ride[\"gender\"].astype(str).str.lower().str.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69df7cd1",
   "metadata": {},
   "source": [
    "##### Extract Stations' Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d04204fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_stations = ride[[\n",
    "    \"start_station_id\",\n",
    "    \"start_station_name\",\n",
    "    \"start_station_longitude\",\n",
    "    \"start_station_latitude\"\n",
    "    ]].rename(columns={\n",
    "        \"start_station_id\": \"station_id\",\n",
    "        \"start_station_name\": \"station_name\",\n",
    "        \"start_station_longitude\": \"station_longitude\",\n",
    "        \"start_station_latitude\": \"station_latitude\"\n",
    "})\n",
    "\n",
    "end_stations = ride[[\n",
    "    \"end_station_id\",\n",
    "    \"end_station_name\",\n",
    "    \"end_station_longitude\",\n",
    "    \"end_station_latitude\"\n",
    "    ]].rename(columns={\n",
    "        \"end_station_id\": \"station_id\",\n",
    "        \"end_station_name\": \"station_name\",\n",
    "        \"end_station_longitude\": \"station_longitude\",\n",
    "        \"end_station_latitude\": \"station_latitude\"\n",
    "})\n",
    "\n",
    "stations = (\n",
    "    pd.concat([start_stations, end_stations])\n",
    "    .dropna(subset=[\"station_id\"])\n",
    "    .drop_duplicates(subset=[\"station_id\"])\n",
    "    .reset_index(drop=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e46b44f",
   "metadata": {},
   "source": [
    "##### Extract Trips' Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe0fbcff",
   "metadata": {},
   "outputs": [],
   "source": [
    "trips = ride[[\n",
    "    \"trip_duration\",\n",
    "    \"start_time\",\n",
    "    \"stop_time\",\n",
    "    \"start_station_id\",\n",
    "    \"end_station_id\",\n",
    "    \"bike_id\",\n",
    "    \"user_type\",\n",
    "    \"birth_year\",\n",
    "    \"gender\"\n",
    "]].copy()\n",
    "\n",
    "trips.insert(0, \"trip_id\", range(1, len(trips) + 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45f4154e",
   "metadata": {},
   "source": [
    "##### Extract Weather Stations' Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62e11f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_station = weather[[\n",
    "    \"weather_station_id\",\n",
    "    \"weather_station_name\"\n",
    "]].drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad6d422e",
   "metadata": {},
   "source": [
    "##### Extract Weather's Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f657bde",
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_data = weather.drop(columns=[\"weather_station_name\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a38c417",
   "metadata": {},
   "source": [
    "##### Schema Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87d729ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_schema(df, expected_cols, table_name):\n",
    "\n",
    "    if set(df.columns) != set(expected_cols):\n",
    "        raise ValueError(f\"Schema mismatch for table '{table_name}'\")\n",
    "\n",
    "\n",
    "validate_schema(stations, [\"station_id\", \"station_name\", \"station_longitude\", \\\n",
    "                           \"station_latitude\"], \"stations\")\n",
    "\n",
    "\n",
    "validate_schema(trips, [\"trip_id\", \"trip_duration\", \"start_time\", \"stop_time\", \\\n",
    "                        \"start_station_id\", \"end_station_id\", \"bike_id\", \\\n",
    "                        \"user_type\", \"birth_year\", \"gender\"], \"trips\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56ecbe0b",
   "metadata": {},
   "source": [
    "##### Load Data Into SQLITE\n",
    "\n",
    "The database and the tables can be created by running the *setupdb.py* file once or importing the *init_db()* function from it and call it once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f0eb051",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from setupdb import init_db\n",
    "# init_db()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce4146f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = sqlite3.connect(DB_PATH)\n",
    "\n",
    "\n",
    "stations.to_sql(\"stations\", conn, if_exists=\"append\", index=False)\n",
    "trips.to_sql(\"trips\", conn, if_exists=\"append\", index=False)\n",
    "weather_station.to_sql(\"weather_station\", conn, if_exists=\"append\", index=False)\n",
    "weather_data.to_sql(\"weather_data\", conn, if_exists=\"append\", index=False)\n",
    "\n",
    "\n",
    "conn.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
